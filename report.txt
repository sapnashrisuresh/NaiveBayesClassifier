Text Classification (Spam and Sentiment Data) using Naive Bayesian Classification.

- Split the labeled data using 75% for training and 25% for development and report the precision, recall and F1 score for SPAM, HAM, POSITIVE and NEGATIVE for the classifier.

Pos/Neg (75-25)
Values for Sentiment (POSITIVE):
Precision = 86.66899930020993  Recall = 80.03231017770598  F-Score= 83.2185452712918

Values for Sentiment (NEGATIVE):
Precision = 81.78066037735849  Recall = 87.92393026941363  F-Score= 84.74110279517336

Spam/Ham (75-25)
Values for Spam Data (SPAM):
Precision = 97.75768535262206  Recall = 98.6856516976999  F-Score= 98.21947674418605

Values for Spam Data (HAM):
Precision = 98.7078248384781  Recall = 97.79516358463727  F-Score= 98.24937477670598

========================================================================================

Files submitted :

nblearn.py
nbclassify.py


Additional files submitted:
preprocess_imdb.py
preprocess_email.py
performanceMeasure.py

**********************************************************************************************

Steps to execute each model on the given data and pre-processing involved:

1. Naive bayes

Sentiment Analysis:

Files provided in HW: 
labeledBow.feat-training data file
sentiment_test.feat - test data file

Pre-processing the Sentiment data:
python3 preprocess_imdb.py labeledBow.feat sentiment_test.feat

	1. Converts label of each rating : >= 7 (positive) or <= 4 (negative) - sentiment.nb.in
	2. Add 1 to each feature for the training and test data - sentiment_test_update.feat
	3. Split the dataset randomly into two files (75% and 25%) — sentiment.train75.in, sentiment.train25.in
	4. Process each of those into development files without label - sentiment.test75.in, sentiment.test25.in
	5. Store the labels in separate files to compute the performance measure of the model - sentiment.reference75.out, sentiment.reference25.out

After preprocessing, execute the following commands to generate models and output for each stage. Check the performance measure for each stage.

python3 nblearn.py sentiment.train75.in sentiment.nb.model
python3 nbclassify.py sentiment.nb.model sentiment.test25.in > sentiment.nb.out
python3 performanceMeasure.py sentiment.reference25.out sentiment.nb.out 1

python3 nblearn.py sentiment.train25.in sentiment.nb.model
python3 nbclassify.py sentiment.nb.model sentiment.test75.in > sentiment.nb.out
python3 performanceMeasure.py sentiment.reference75.out sentiment.nb.out 1

python3 nblearn.py sentiment.nb.in sentiment.nb.model
python3 nbclassify.py sentiment.nb.model sentiment_test_update.feat > sentiment.nb.out


SPAM filtering:

Files provided in HW: 
enron.vocab - vocabulary file for email data
Bunch of EMAIL files. The program recursively looks for text files from the current directory and writes them into one training file with feature-value pairs. For test dataset, the program assumes “spam_or_ham_test” folder exists under the current directory.
Assumption: All the training email data files end with “am.txt” and SPAM-files end with “spam.txt” and HAM-files with “ham.txt”.

Pre-processing the Sentiment data:
python3 preprocess_email.py enron.vocab spam.nb.in spam_test.nb.in

	1. Convert the email data to a single file with features and values with labels - spam.nb.in
	2. Convert the email test data to a single file with features and values - spam_test.nb.in
	3. Split the dataset randomly into two files (75% and 25%) — spam.train75.in, spam.train25.in
	4. Process each of those into development files without label - spam.test75.in, spam.test25.in
	5. Store the labels in separate files to compute the performance measure of the model - spam.reference75.out, spam.reference25.out

After preprocessing, execute the following commands to generate models and output for each stage. Check the performance measure for each stage.

python3 nblearn.py spam.train75.in spam.nb.model
python3 nbclassify.py spam.nb.model spam.test25.in > spam.nb.out
python3 performanceMeasure.py spam.reference25.out spam.nb.out 2

python3 nblearn.py spam.train25.in spam.nb.model
python3 nbclassify.py spam.nb.model spam.test75.in > spam.nb.out
python3 performanceMeasure.py spam.reference75.out spam.nb.out 2

python3 nblearn.py spam.nb.in spam.nb.model
python3 nbclassify.py spam.nb.model spam_test.nb.in  > spam.nb.out


2. Performance Measure

Used to measure precision, recall and F-score to measure the performance of each model. The values are displayed as % for better comparison between models.

To run the script, provide the reference results and the output of the current model, followed by an option (1:Sentiment Analysis, 2:SPAM filtering,
3:SVM, 4:MegaM)

python3 performanceMeasure.py sentiment.reference25.out sentiment25.nb.out 1

It displays the performance result on the console.

